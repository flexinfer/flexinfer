# FlexInfer

<p align="center">
  <img src="logo.png" width="400">
</p>

> **Kubernetes operator + scheduler plugin that routes LLM inference to the best mix of AMD, NVIDIA, or CPU nodes‚Äîautomatically.**

[![CI](https://github.com/crb2nu/flexinfer/actions/workflows/ci.yml/badge.svg)](https://github.com/crb2nu/flexinfer/actions/workflows/ci.yml)
[![License](https://img.shields.io/badge/License-Apache-2.0-blue.svg)](LICENSE)

FlexInfer closes the gap between ‚ÄúI have whatever GPUs are lying around‚Äù and ‚ÄúI want my models to run fast, cheaply, and with no manual node labels.‚Äù
Home-labbers and on-prem teams can declare **one** `ModelDeployment` CRD; FlexInfer discovers the cluster‚Äôs capabilities, benchmarks each model once, and schedules pods to the cheapest node that meets their throughput SLO.

---

## ‚ú® Features

* **Zero-touch GPU discovery** ‚Äì Detects CUDA, ROCm, VRAM, FP16/INT4, & temperature via a lightweight node agent.
* **Auto-benchmark & caching** ‚Äì Runs a micro-benchmark per model √ó device class; stores a shared model cache so disks aren‚Äôt littered with duplicates.
* **Throughput-aware scheduling** ‚Äì A scheduler extender selects nodes based on benchmarked *tokens/s* and live utilization.
* **Plug-in backends** ‚Äì Works with Ollama, vLLM, TensorRT-LLM (bring-your-own container image).
* **Observability out of the box** ‚Äì Exposes Prometheus metrics (`tokens_per_second`, `latency_p95`, `gpu_temperature`) and ships a Grafana dashboard.
* **Tiny footprint** ‚Äì < 20 MB binary, no Istio, no sidecar explosion‚Äîperfect for home labs and edge clusters.

---

## üöÄ Quick start

```bash
# 1. Create a local multi-node cluster (kind + containerd runtime-class support)
kind create cluster --config hack/kind-mixed-gpu.yaml

# 2. Install FlexInfer CRDs & controller
helm repo add flexinfer https://flexinfer.github.io/charts
helm install flexinfer flexinfer/flexinfer --namespace flexinfer-system --create-namespace

# 3. Deploy your first model
kubectl apply -f examples/llama3-8b.yaml

# 4. Watch the pods land on the optimal node
kubectl get pods -l flexinfer.ai/model=llama3-8b -o wide
```

## üìö Getting Started

To get started with FlexInfer, you need to have a Kubernetes cluster with GPU nodes. You can use any cloud provider or a local cluster.

### Prerequisites

* A Kubernetes cluster with GPU nodes (AMD or NVIDIA).
* `kubectl` installed and configured to connect to your cluster.
* `helm` installed.

### Installation

1. **Add the FlexInfer Helm repository:**

   ```bash
   helm repo add flexinfer https://flexinfer.github.io/charts
   ```

2. **Install the FlexInfer operator:**

   ```bash
   helm install flexinfer flexinfer/flexinfer --namespace flexinfer-system --create-namespace
   ```

3. **Verify the installation:**

   ```bash
   kubectl get pods -n flexinfer-system
   ```

   You should see the FlexInfer controller manager running.

### Deploying a Model

Once the operator is running, you can deploy a model using the `ModelDeployment` CRD. Here is an example of a `ModelDeployment` for `llama3-8b`:

```yaml
apiVersion: ai.flexinfer/v1alpha1
kind: ModelDeployment
metadata:
  name: llama3-8b
spec:
  backend: ollama
  model: llama3:8b
  replicas: 1
```

Save this to a file called `llama3-8b.yaml` and apply it to your cluster:

```bash
kubectl apply -f llama3-8b.yaml
```

The FlexInfer operator will automatically detect the best node to run the model on, based on the available resources and the model's requirements.
---

üìÇ Repository layout

.
‚îú‚îÄ‚îÄ api/               # CRD types and validation
‚îú‚îÄ‚îÄ cmd/               # flexinfer-manager main()
‚îú‚îÄ‚îÄ controllers/       # Reconciler logic
‚îú‚îÄ‚îÄ scheduler/         # Scheduler extender (gRPC)
‚îú‚îÄ‚îÄ agents/            # Node agent & benchmarker
‚îúÔøΩÔøΩÔøΩ‚îÄ charts/            # Helm chart
‚îî‚îÄ‚îÄ examples/          # Sample ModelDeployment manifests

Architecture overview:

```mermaid
graph TD
    subgraph Node
        Node_Agent[Node Agent]
    end

    subgraph Control Plane
        ModelDeployment(ModelDeployment)
        FlexInfer_Ctrl[FlexInfer Ctrl]
        Benchmarker_Job[Benchmarker Job]
        ConfigMap[ConfigMap]
        Scheduler_Extender[Scheduler Extender]
    end

    Node_Agent -- labels --> FlexInfer_Ctrl
    ModelDeployment -- deploys --> FlexInfer_Ctrl
    FlexInfer_Ctrl -- creates --> Benchmarker_Job
    Benchmarker_Job -- benchmarks --> ConfigMap
    ConfigMap -- scores nodes --> Scheduler_Extender
    FlexInfer_Ctrl -- uses --> Scheduler_Extender
```

A deeper dive into each component lives in AGENTS.md.

---

## ToDo

- [ ] Add support for more LLM backends (e.g., TGI, SGL)
- [ ] Implement a more sophisticated scoring algorithm
- [ ] Add support for multi-GPU nodes
- [ ] Add support for more cloud providers
- [ ] Add more tests
- [ ] Add more documentation
- [ ] Add more examples
- [ ] Add a CI/CD pipeline
- [ ] Add a proper logo
- [ ] Add a website

---

‚öôÔ∏è Requirements

* Kubernetes 1.26+ (tested on K3s, MicroK8s, Kind, EKS)
* Linux nodes with:
  * AMD ROCm 5.7+ or NVIDIA CUDA 12.4+ driver
  * Container runtime that supports GPU runtime classes (containerd ‚â• 1.6)
* Optional: Prometheus Operator for full metrics
---

üìà Metrics & dashboards

| Metric | Description |
|---|---|
| `flexinfer_tokens_per_second` | Real-time throughput per pod |
| `flexinfer_latency_p95_seconds` | p95 end-to-end latency |
| `flexinfer_gpu_temperature_celsius` | GPU core temp per device |

Import hack/grafana/flexinfer.json into Grafana to get an instant overview of cluster-wide inference performance.
---

üõ†Ô∏è Development

make docker-build docker-push IMG=harbor.lan/library/flexinfer:dev
kind load docker-image harbor.lan/library/flexinfer:dev
make deploy

Tests: go test ./...
Lint: golangci-lint run
---

ü§ù Contributing

We love contributions of all kinds‚Äîcode, docs, bug reports. Start by reading our CONTRIBUTING.md and look for issues tagged good first issue.

---

üìú License

This project is licensed under the Apache License 2.0. See the [LICENSE](LICENSE) file for details.
